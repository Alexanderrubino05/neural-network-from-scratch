{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37198973-e60f-4464-b728-6bc9ab26449e",
   "metadata": {},
   "source": [
    "# Creating layer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5412a422-74c3-4a81-88b0-d30e36bc9b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "968e41a5-6a8c-4ebd-9dd1-200ed1376043",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "nnfs.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7253df30-34e2-4820-834f-56ee941df73a",
   "metadata": {},
   "source": [
    "# Neural layer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4c4f393-ee4f-47e7-b247-6be26cc02a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.10 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs # Remember inputs for backpropagation\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "\n",
    "        # Calculate gradient on inputs\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a495e9-44a2-4d07-9cd5-5d759e22b5a4",
   "metadata": {},
   "source": [
    "# Creating ReLu Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f6da45a-93af-4be7-9058-052cedb60647",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs # Remember inputs for backpropagation\n",
    "        \n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        \n",
    "        # Calculate gradient on inputs\n",
    "        self.dinputs[self.inputs <= 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87d1567-6910-43f8-aec6-b3b10fb19f26",
   "metadata": {},
   "source": [
    "# Softmax Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bddefc14-f001-450d-a1d7-32a1dcfdaa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs # Remember inputs for backpropagation\n",
    "        \n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        \n",
    "        self.output = probabilities\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126bdca2-4ca8-42a5-9aea-ac71f133baaf",
   "metadata": {},
   "source": [
    "# Categorical Cross-Entropy Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "438c0427-0ea7-42cf-b9ce-4dd96f09b2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:    \n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        \n",
    "        return data_loss\n",
    "\n",
    "class Loss_CategoricalCrossEntropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "\n",
    "        # Check shape of targets\n",
    "        if len(y_true.shape) == 1:\n",
    "            # Data is [0,1,0]...\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "        elif len(y_true.shape) == 2:\n",
    "            # Data is [[1,0],[0,1],[1,0]]\n",
    "            correct_confidences = np.sum(y_pred_clipped*y_true, axis=1)\n",
    "        \n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        # Turn labelse into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        # Calculate gradient on inputs\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        self.dinputs = self.dinputs / samples # Normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dac132-1da4-4ee7-940c-a3af81577979",
   "metadata": {},
   "source": [
    "# Softmax combined with Categorical Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b93d0336-62bb-46a8-9134-b31b873978cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossEntropy()\n",
    "\n",
    "    def forward(self, inputs, y_true):\n",
    "        self.activation.forward(inputs)\n",
    "        self.output = self.activation.output\n",
    "\n",
    "        # Calculate loss\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        # Reshape targets if [[0,1], [1,0]]\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        # Calculate gradient on inputs\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f094869-b08d-40b4-992c-82444f7a4a2e",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4ec1b57-898c-45c7-9aab-887fd8c4e21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_SGD():\n",
    "    def __init__(self, learning_rate=1, decay=0, momentum=0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1 / (1 + self.decay * self.iterations))\n",
    "    \n",
    "    def update_params(self, layer):\n",
    "        if self.momentum:\n",
    "            # Uses momentum\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "            # Weight updates\n",
    "            weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "\n",
    "            # Bias updates\n",
    "            bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "            \n",
    "        else:\n",
    "            layer.weights += -self.current_learning_rate * layer.dweights\n",
    "            layer.biases += -self.current_learning_rate * layer.dbiases\n",
    "\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ef1e02-a5e1-4348-b062-4b08466f7757",
   "metadata": {},
   "source": [
    "# Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71da3ada-b091-4877-9da1-d2ec3595c04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_Adam:\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "    \n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self): \n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "    \n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer does not contain cache arrays, create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "            \n",
    "        # Update momentum  with current gradients\n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
    "        \n",
    "        # Get corrected momentum\n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        \n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
    "        \n",
    "        # Get corrected cache\n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        \n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self): \n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fd8c92-4bbf-4942-9447-efd2ccabb925",
   "metadata": {},
   "source": [
    "# Zalando Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3ba9878-9ebb-4b2b-a405-933653faee40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a684b48-2e79-4785-8009-78e3d9dc9c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_dataset(dataset, path):\n",
    "    labels = sorted(os.listdir(os.path.join(path, dataset))[1:], key=int)\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for label in labels:        \n",
    "        for file in os.listdir(os.path.join(path, dataset, label)):\n",
    "            if \".png\" in file:\n",
    "                # Read image\n",
    "                image = cv2.imread(os.path.join(path, dataset, label, file), cv2.IMREAD_UNCHANGED)            \n",
    "                \n",
    "                X.append(image)\n",
    "                y.append(label)\n",
    "        \n",
    "    return np.array(X), np.array(y).astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bd41473-ff45-4c0e-ae38-253dd48e810a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_mnist(path):\n",
    "    X, y = load_mnist_dataset('train', path)\n",
    "    X_test, y_test = load_mnist_dataset('test', path)\n",
    "\n",
    "    return X, y, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896a56e0-0f8b-495e-9b63-7fb8da97c777",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92e7dd89-a0c5-4e7c-a2ab-9d79f9563e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "X, y, X_test, y_test = create_data_mnist('fashion_mnist_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01f05556-40c6-45a7-b143-42be72383380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "X = (X.astype(np.float32) - 127.5) / 127.5\n",
    "X_test = (X_test.astype(np.float32) - 127.5) / 127.5\n",
    "\n",
    "# Convert X to an array\n",
    "X = X.reshape(X.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d752866-d58f-49d1-bbd6-b6996e15bb9e",
   "metadata": {},
   "source": [
    "### Shuffle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25c307c2-5283-4549-baf3-d50d0850e26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = np.array(range(X.shape[0]))\n",
    "np.random.shuffle(keys)\n",
    "\n",
    "X = X[keys]\n",
    "y = y[keys]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe384d60-72f2-4a8d-9dd5-a608698e7e8e",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1085fd8b-9fc9-4db8-82fe-00806e8db9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden layer\n",
    "layer1 = Layer_Dense(X.shape[1], 128)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Output layer\n",
    "layer2 = Layer_Dense(128, 128)\n",
    "activation2 = Activation_ReLU()\n",
    "\n",
    "# Output layer\n",
    "layer3 = Layer_Dense(128, 10)\n",
    "activation3 = Activation_Softmax()\n",
    "\n",
    "# Loss\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = Optimizer_Adam(decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4e420393-0699-43ca-ac5e-6a0141159697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "step: 0, acc: 0.086, loss: 3.040, lr: 0.001\n",
      "step: 100, acc: 0.820, loss: 0.520, lr: 0.0009099181073703368\n",
      "step: 200, acc: 0.805, loss: 0.499, lr: 0.0008340283569641367\n",
      "step: 300, acc: 0.844, loss: 0.529, lr: 0.0007698229407236335\n",
      "step: 400, acc: 0.844, loss: 0.392, lr: 0.0007147962830593281\n",
      "epoch: 0, acc: 0.815, loss: 0.518, lr: 0.000681198910081744\n",
      "epoch: 1\n",
      "step: 0, acc: 0.859, loss: 0.316, lr: 0.000681198910081744\n",
      "step: 100, acc: 0.875, loss: 0.318, lr: 0.0006377551020408163\n",
      "step: 200, acc: 0.867, loss: 0.348, lr: 0.0005995203836930455\n",
      "step: 300, acc: 0.836, loss: 0.457, lr: 0.0005656108597285068\n",
      "step: 400, acc: 0.883, loss: 0.345, lr: 0.0005353319057815846\n",
      "epoch: 1, acc: 0.867, loss: 0.369, lr: 0.0005162622612287042\n",
      "epoch: 2\n",
      "step: 0, acc: 0.867, loss: 0.278, lr: 0.0005162622612287042\n",
      "step: 100, acc: 0.898, loss: 0.290, lr: 0.0004909180166912126\n",
      "step: 200, acc: 0.883, loss: 0.298, lr: 0.0004679457182966776\n",
      "step: 300, acc: 0.859, loss: 0.414, lr: 0.0004470272686633884\n",
      "step: 400, acc: 0.898, loss: 0.324, lr: 0.00042789901583226365\n",
      "epoch: 2, acc: 0.882, loss: 0.329, lr: 0.0004156275976724854\n",
      "epoch: 3\n",
      "step: 0, acc: 0.906, loss: 0.250, lr: 0.0004156275976724854\n",
      "step: 100, acc: 0.883, loss: 0.251, lr: 0.0003990422984836393\n",
      "step: 200, acc: 0.898, loss: 0.285, lr: 0.00038372985418265546\n",
      "step: 300, acc: 0.852, loss: 0.376, lr: 0.0003695491500369549\n",
      "step: 400, acc: 0.898, loss: 0.309, lr: 0.0003563791874554526\n",
      "epoch: 3, acc: 0.890, loss: 0.305, lr: 0.00034782608695652176\n",
      "epoch: 4\n",
      "step: 0, acc: 0.906, loss: 0.233, lr: 0.00034782608695652176\n",
      "step: 100, acc: 0.906, loss: 0.220, lr: 0.00033613445378151256\n",
      "step: 200, acc: 0.898, loss: 0.263, lr: 0.0003252032520325203\n",
      "step: 300, acc: 0.852, loss: 0.361, lr: 0.00031496062992125983\n",
      "step: 400, acc: 0.898, loss: 0.298, lr: 0.0003053435114503817\n",
      "epoch: 4, acc: 0.897, loss: 0.287, lr: 0.0002990430622009569\n",
      "epoch: 5\n",
      "step: 0, acc: 0.914, loss: 0.220, lr: 0.0002990430622009569\n",
      "step: 100, acc: 0.914, loss: 0.204, lr: 0.00029036004645760743\n",
      "step: 200, acc: 0.898, loss: 0.246, lr: 0.00028216704288939055\n",
      "step: 300, acc: 0.852, loss: 0.345, lr: 0.000274423710208562\n",
      "step: 400, acc: 0.898, loss: 0.283, lr: 0.0002670940170940171\n",
      "epoch: 5, acc: 0.901, loss: 0.273, lr: 0.00026226068712300026\n",
      "epoch: 6\n",
      "step: 0, acc: 0.922, loss: 0.204, lr: 0.00026226068712300026\n",
      "step: 100, acc: 0.930, loss: 0.193, lr: 0.0002555583950932788\n",
      "step: 200, acc: 0.898, loss: 0.232, lr: 0.00024919013207077\n",
      "step: 300, acc: 0.852, loss: 0.332, lr: 0.00024313153415998057\n",
      "step: 400, acc: 0.914, loss: 0.271, lr: 0.00023736055067647758\n",
      "epoch: 6, acc: 0.906, loss: 0.262, lr: 0.00023353573096683791\n",
      "epoch: 7\n",
      "step: 0, acc: 0.930, loss: 0.189, lr: 0.00023353573096683791\n",
      "step: 100, acc: 0.930, loss: 0.183, lr: 0.00022820629849383846\n",
      "step: 200, acc: 0.906, loss: 0.220, lr: 0.00022311468094600624\n",
      "step: 300, acc: 0.867, loss: 0.329, lr: 0.00021824530772588386\n",
      "step: 400, acc: 0.906, loss: 0.260, lr: 0.0002135839384878257\n",
      "epoch: 7, acc: 0.910, loss: 0.252, lr: 0.00021048200378867611\n",
      "epoch: 8\n",
      "step: 0, acc: 0.938, loss: 0.183, lr: 0.00021048200378867611\n",
      "step: 100, acc: 0.938, loss: 0.176, lr: 0.00020614306328592044\n",
      "step: 200, acc: 0.922, loss: 0.209, lr: 0.00020197939810139365\n",
      "step: 300, acc: 0.859, loss: 0.326, lr: 0.00019798059790140566\n",
      "step: 400, acc: 0.914, loss: 0.244, lr: 0.00019413706076490004\n",
      "epoch: 8, acc: 0.913, loss: 0.243, lr: 0.00019157088122605365\n",
      "epoch: 9\n",
      "step: 0, acc: 0.938, loss: 0.174, lr: 0.00019157088122605365\n",
      "step: 100, acc: 0.945, loss: 0.172, lr: 0.00018796992481203006\n",
      "step: 200, acc: 0.922, loss: 0.198, lr: 0.00018450184501845018\n",
      "step: 300, acc: 0.859, loss: 0.321, lr: 0.00018115942028985505\n",
      "step: 400, acc: 0.914, loss: 0.238, lr: 0.00017793594306049823\n",
      "epoch: 9, acc: 0.915, loss: 0.236, lr: 0.00017577781683951485\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    print(f'epoch: {epoch}')  \n",
    "\n",
    "    batch_size = 128\n",
    "    train_steps = len(X) // batch_size\n",
    "    if train_steps * batch_size < len(X):\n",
    "        train_steps += 1\n",
    "\n",
    "    # Epoch loss and accuracy\n",
    "    combined_loss = 0\n",
    "    combined_accuracy = 0\n",
    "    count = 0\n",
    "\n",
    "    for step in range(train_steps):\n",
    "        batch_X = X[step*batch_size:(step+1)*batch_size]\n",
    "        batch_y = y[step*batch_size:(step+1)*batch_size]\n",
    "        \n",
    "        # Forward pass on first layer\n",
    "        layer1.forward(batch_X)\n",
    "        activation1.forward(layer1.output)\n",
    "    \n",
    "        # Forward pass on second layer\n",
    "        layer2.forward(activation1.output)\n",
    "        activation2.forward(layer2.output)\n",
    "    \n",
    "        # Forward pass on third layer\n",
    "        layer3.forward(activation2.output)\n",
    "    \n",
    "        # Calculate loss for step\n",
    "        step_loss = loss_activation.forward(layer3.output, batch_y)\n",
    "        \n",
    "        # Calculate accuracy for step\n",
    "        predictions = np.argmax(loss_activation.output, axis=1)\n",
    "        if len(y.shape) == 2:\n",
    "            batch_y = np.argmax(batch_y, axis=1)\n",
    "        step_accuracy = np.mean(predictions == batch_y)       \n",
    "\n",
    "        if not step % 100:\n",
    "            print(f'step: {step}, acc: {step_accuracy:.3f}, loss: {step_loss:.3f}, lr: {optimizer.current_learning_rate}')\n",
    "        \n",
    "        combined_loss += step_loss\n",
    "        combined_accuracy += step_accuracy\n",
    "        count += 1\n",
    "        \n",
    "        # Backpropogation\n",
    "        loss_activation.backward(loss_activation.output, batch_y)\n",
    "        layer3.backward(loss_activation.dinputs)\n",
    "        activation2.backward(layer3.dinputs)\n",
    "        layer2.backward(activation2.dinputs)\n",
    "        activation1.backward(layer2.dinputs)\n",
    "        layer1.backward(activation1.dinputs)\n",
    "    \n",
    "        # Optimize weights and bias\n",
    "        optimizer.pre_update_params()\n",
    "        optimizer.update_params(layer1)\n",
    "        optimizer.update_params(layer2)\n",
    "        optimizer.update_params(layer3)\n",
    "        optimizer.post_update_params()\n",
    "\n",
    "    # Calculate loss and accuracy for epoch\n",
    "    loss = combined_loss / count\n",
    "    acc = combined_accuracy / count\n",
    "    \n",
    "    if not epoch % 1:\n",
    "        print(f'epoch: {epoch}, acc: {acc:.3f}, loss: {loss:.3f}, lr: {optimizer.current_learning_rate}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1232974-b38c-4252-9cba-a0a88edc6bfc",
   "metadata": {},
   "source": [
    "# The networks prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b87214d-b69f-45d0-8be3-b28dd23e29ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label(image):\n",
    "    # Forward pass on first layer\n",
    "    layer1.forward(image)\n",
    "    activation1.forward(layer1.output)\n",
    "    \n",
    "    # Forward pass on second layer\n",
    "    layer2.forward(activation1.output)\n",
    "    activation2.forward(layer2.output)\n",
    "    \n",
    "    # Forward pass on third layer\n",
    "    layer3.forward(activation2.output)\n",
    "    activation3.forward(layer3.output)\n",
    "    return activation3.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "67b1c814-11b8-44a6-b9ee-7854b4a13d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = X_test[1]\n",
    "\n",
    "prediction = predict_label(image)\n",
    "label = np.argmax(prediction)\n",
    "\n",
    "plt.imshow(image.reshape(28, 28), cmap=\"Greens\")\n",
    "plt.show()\n",
    "\n",
    "#print(label, get_description(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d8e038d-ccec-4199-b3e0-def0b863a405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_description(label):\n",
    "    label_descriptions = {\n",
    "        0: \"T-shirt\",\n",
    "        1: \"Trouser\",\n",
    "        2: \"Pullover\",\n",
    "        3: \"Dress\",\n",
    "        4: \"Coat\",\n",
    "        5: \"Sandal\",\n",
    "        6: \"Shirt\",\n",
    "        7: \"Sneaker\",\n",
    "        8: \"Bag\",\n",
    "        9: \"Ankle boot\"\n",
    "    }\n",
    "    return label_descriptions.get(label, \"Unknown label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e836200-b576-4f65-9a82-608675285f88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1118cf-ec63-4143-98f2-16b679cf2b0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
